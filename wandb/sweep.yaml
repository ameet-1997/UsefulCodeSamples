# Run vanilla baseline model on RCV1
# No bells and whistles

program: ../run_rcv1_baseline.py
command:
- python
- ${program}
- ${args}
- --do_train
- --do_eval
- --overwrite_output_dir

# First define the defaults
method: grid
metric:
  name: eval/f1
  goal: maximize
parameters:

  model_name_or_path:
    value: "roberta-base"

  cache_dir:
    value: "/n/fs/nlp-asd/asd/asd/BERT_Embeddings_Test/BERT_Embeddings_Test/global_data/transformer_models"

  max_seq_length:
    value: 512
  
  train_file:
    value: "/n/fs/nlp-asd/asd/asd/Projects/SemanticLabelEmbeddings/data/RCV1/rcv1_processed/rcv1_train.json"
    
  validation_file:
    value: "/n/fs/nlp-asd/asd/asd/Projects/SemanticLabelEmbeddings/data/RCV1/rcv1_processed/rcv1_valid.json"

  per_device_train_batch_size:
    value: 24

  per_device_eval_batch_size:
    value: 24    

  learning_rate:
    values: [2e-5, 5e-5, 1e-4, 5e-4]

  lr_scheduler_type:
    value: "constant"

  num_train_epochs:
    value: 1

  output_dir:
    value: "/n/fs/nlp-asd/asd/asd/Projects/SemanticLabelEmbeddings/model_outputs/debug"

  save_steps:
    value: -1

  evaluation_strategy:
    value: "steps"

  eval_steps:
    value: 5000